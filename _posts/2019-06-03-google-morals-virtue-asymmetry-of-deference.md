---
layout: post
title: "Okay Google, should I punch my friend?"
date: 2019-06-03
excerpt_separator: <!--cut-->
---
## The Short Version
Forming beliefs on moral issues by deferring to someone or something else is problematic because these beliefs don't integrate with our moral character.

<!--cut-->

## The Puzzle
{: id="anchor"}
Imagine that Google has just released their latest app entitled Google Morals. Just like how you can ask Google how to get somewhere, now with Google Morals you can ask what to do for any moral issue. Should I eat this steak? Should I be an organ donor? Google Morals will answer any of these questions with a simple "Yes" or "No."

To me, immediately it seems like there's something off about using Google Morals. I do defer to Google on other things like the radius of the Earth or how to get to the airport, but something seems different with moral issues.

Robert J. Howell provides a novel account of what exactly is wrong about deferring on moral issues.[^Howell_paper] We'll get to this in a minute.

[^Howell_paper]: "Google Morals, Virtue, and the Asymmetry of Deference," *Nous* 48:3 (2014):p.389-415

## The Definition of Moral Deference

A significant part of the paper is devoted to talking about what exactly moral deference is and its many types. Some of it is technical so I leave it out to make my job easier. Howell thinks that deference includes (i) **forming a belief in some statement p** based on another person's believing p and (ii) **sustaining that belief in p based on another person's believing p**. So we're not using whatever definition of deference that's in the dictionary.

Why should we use Howell's definition of deference? Howell thinks that this definition covers at least the essential aspects of deference; we believe and continue to believe p because someone told us so.

Let's take a look at how this definition works. I might not know what the closest planet to the Sun is, but I know my friend who's really into Astronomy does know. I ask my friend, and she tells me it's Mercury. That's p, the statement that Mercury is the closest planet to the Sun. Now, I believe p because my friend told me so. I haven't actually looked it up on Wikipedia or seen it for myself so I'm believing p based on what my friend believes.[^no_lies]

[^no_lies]: I'm assuming here that my friend telling me p also means that she believes p. I don't think she would lie to me about this.

The next day someone asks me what the closest planet to the Sun is (it's some new trend I guess). I tell them it's Mercury, and when they ask me how I know that I tell them my friend who's into Astronomy told me. This is (ii) of Howell's definition. I continue to believe that Mercury is the closest planet to the Sun **solely** based on what my friend told me.[^sustain_how_long] I myself don't understand why Mercury is the closest planet to the Sun. You might think this is problematic, but Howell thinks the bigger problem is with deferring on moral issues.

[^sustain_how_long]: Howell seems to mean that deferring on p means that we sustain our belief in p forever based on someone else's word, but he also doesn't seem to mean this which confuses me. See his discussion at the end of the paper on moral development for this.

Imagine that I'm discussing with my friend over the phone whether I should register as an organ donor, and I just can't reach a conclusion. After I hang up, I ask Google Morals whether I should register as an organ donor. Let's assume Google Morals says that I should register as an organ donor. So now I believe I should register as an organ donor because Google Morals told me so.

Already, this might seem off to you, but wait it gets worse! The next day my friend asks me what I ended up doing, and I tell him I registered as an organ donor. He asks me why, and I tell him because Google Morals told me so. Wait, what! This seems very wrong. You might think that deferring to your friend on whether Mercury is the closest planet to the Sun is okay at best, but deferring to Google Morals on whether I should be an organ donor seems worse. If your intuition doesn't line up with mine, Howell's account gives some reasons to think moral deference is especially bad.

## What's wrong with moral deference?

Howell looks at several possible explanations of why moral deference is bad, but he thinks that only his account gives the full story of what's at the heart of the problem.

I won't go through all of the possible explanations, but here are at least the ones I thought touched on an important aspect of the problem.

### Understanding
If you think about it, Google Morals only tells you what you should do and nothing more. It doesn't tell you why you should do it or what sort of reasoning led to its conclusion. I might *know* what I should do, but what's wrong is that I don't understand why I should either register as an organ donor or not. The *why* might include the benefits of donating my organs or my right to what happens to my body. If I don't understand why I should register as an organ donor, how would I know what to do in a similar case? Would I always ask Google Morals what to do? Our lack of understanding explains the problem with moral deference.

<!-- Google rolls out a new feature for Google Morals that now also tells you why you should do what it tells you to do. (Howell's argument for the lack of distinction between understanding why and knowing why)

But I don't think you're really understanding why even if you know why. You also need to internalize the why to really understand why. -->

Howell argues that this argument only partially explains what's wrong with moral deference. There are cases where we understand why we should do what Google Morals tells us to do, but it still seems problematic to defer. For example, we may defer to Google Morals on which moral theory is right, and if it says utilitarianism is right we would understand the reason behind whatever Google Morals tells us to do. For example, we might know why Google Morals says we should be vegetarians (because it would minimize the suffering of animals), but we still wouldn't know why utilitarianism is correct.

<!-- He uses the examples of Sam, Urkel, Alastair, and Ursula to show this.

Sam understands all the moral theories: utilitarianism, deontology, etc. However, like most of us, he doesn't know which moral theory is the right one. So he asks Google Morals which one is the right one, and let's say Google Morals says it's utilitarianism. Now, whenever Sam is faced with some moral dilemma, he knows what to do since he understands how to apply utilitarianism.

Here, Sam understands why he needs to do some action, but something still seems wrong. This takes care of the problem the understanding account brings up. Sam understands, for example, why he should register as an organ donor. But he doesn't understand why utilitarianism is the correct moral theory. Howell doesn't address this worry so I'm still not convinced that the understanding account is only a partial explanation.

Urkel is unlike us. He defers to Google Morals on what to do, but he can justify whatever Google's answer is. In this way, he understands why, but there is something seriously wrong with Urkel. He defers even on the 'simplest' questions like "Is it wrong to torture babies?" He doesn't have the basic moral intuitions that we all have when confronted with a moral question.

Howell wants to show us that though Sam and Urkel have understanding they still have serious problems so the understanding account cannot give us the full story. I don't actually think this is right because Howell seems to have given us cases of partial understanding.

Alastair is admirable. He doesn't defer to Google Morals and has the intuitions and arguments to back up his moral beliefs. However, he is always wrong. Because we cannot understand something that is false, Howell thinks that Alastair lacks understanding but still has some positive features.

Ursula doesn't have understanding (in any way really). She defers to Google Morals, but nearly all the time she acts morally even without Google's help.

These last two cases are supposed to show us that though Alastair and Ursula don't have understanding they still have some positive attributes.

The point with these four cases is that understanding doesn't give a full story of the problem with moral deference because problems are left unaccounted for and positives are left unaccounted by understanding. -->

### Because you told me so...
Another explanation Howell considers is that we are acting some way only because someone else told us to do so.

Howell thinks there is a missing link here. We act some way because we believe it is the right thing to do, and we believe it is the right thing to do because someone else (who we think is reliable) told us. I don't register as an organ donor simply because Google Morals told me. I register as an organ donor because I think it is the right thing to do, and I believe it's the right thing to do because Google Morals told me.[^same_q]

[^same_q]: I wonder if we could object that what we are trying to say here is that the problem with moral deference is believing something is the right thing to do only because someone else told us so. But Howell might reply here that if we think Google Morals is reliable then there shouldn't be a problem with believing what Google Morals says is the right thing to do.

### It's your job!
The last explanation Howell considers is about doing our job.

Consider Gary the Googler. Gary doesn't trust himself to do the right thing so whenever something comes up he asks Google Morals what to do.

We might think something seems off about what Gary is doing. Howell thinks it has to do with how we think it's everyone's job to figure out what to do (morally speaking). We can't let Google Morals do what is our job.

<!-- On the other hand, we also might think Gary is being responsible. He understands that he is not good at doing the right thing so he defers to a moral expert.

To better understand this second reaction, consider Patty the physicist. It is Patty's job to advance our knowledge of physics. But it is not my job to do physics research. Then, it seems responsible of me to defer to Patty about facts of physics or things in her expertise. After all, I'm not the expert. This seems to be exactly what Gary is doing but for moral truths. -->

Howell thinks that this explanation doesn't actually explain anything since now we're left with the question of why *we* are responsible for figuring out moral truths. We're just posing another question in response to the first question of why moral deference is bad.[^still_interesting]

[^still_interesting]: I'm not convinced by Howell's objection here because it does seem like this question digs deeper. If we are, in fact, responsible for figuring out moral truths, this would nicely explain why moral deference is bad.

## Howell's account
So now that we've considered a list of (supposedly) problematic explanations, let's take a look at Howell's.

In one sentence, Howell thinks the problem with moral deference is that it indicates a lack of moral virtue or makes it harder to acquire moral virtue. In Howell's own words, "the beliefs sustained by deference are largely isolated from the moral character of the agent" (p.402). There's a lot squeezed into that sentence so let's unpack it.

First, moral character is used in much the same way that we talk about someone's character. Moral character has to do with what kinds of virtues we have.

"To have a virtue is to have a reliable disposition to act and feel in certain ways" (p.403). You can think of virtue as being made up of 2 components: the disposition to act a certain way and the disposition to feel a certain way. For example, if I have the virtue of courage, then I (nearly all the time) am able to act and feel courageously.[^circularity]

[^circularity]: This might seem circular because the definition of having the virtue of courage is defined in terms of courage. I think this can be safely ignored for Howell's account so let's assume we know what courage is and what a courageous action is.

The core of Howell's account is that by deferring we don't properly integrate these "virtuous features" (the actions or feelings) with our moral character. This means we only learn what the right thing to do is and don't get any of the other components of having virtue such as the feelings or actions. Then, we don't have the relevant virtue (because we're missing one of the components) and/or it's harder to acquire that virtue in the future (p.403).

For example, I'm walking down the street and I see a homeless man. If I have the virtue of compassion or generosity, I might try to help this man with food, shelter, or money. But it's not just the action. As I give this man some money, I will feel relieved or glad that I'm helping this man. It's also the case that I would almost always do the same thing in a similar situation. This is what it takes to have the virtue. Just the action is not enough.

Now compare this to an alternate situation. I'm walking down the street and I see a homeless man. Not sure about whether I should just pass by or try to help, I ask Google Morals what to do. Google Morals tells me I should help this man. So I also give this man some money like before. But this time I feel torn about giving money. I was going to use that to buy ice cream so I feel frustrated (we might think I'm a jerk for feeling so). In a similar situation, I might try to ignore the man since I know what Google Morals will tell me (even more of a jerk move). In this case, I do not have the virtue of compassion or generosity; I don't feel the right feelings or act generously. Since I **only** act correctly, I don't satisfy all the conditions to integrate the relevant virtue with my moral character.

But moral deference can also make it harder to acquire moral virtue. This is where Howell defers to the understanding account.[^joke] We don't understand the reasons for the belief we get from moral deference so we can't apply it to similar situations we get into. I might learn from Google Morals that I should give money to the homeless man, but if I don't understand why Google Morals told me this I will most likely not do the same in similar, future encounters. This means that acquiring the virtue of compassion or generosity will be more difficult because I can't consistently apply the knowledge I get from moral deference.

[^joke]: I had to do it.

So that's it! I think Howell's virtue account is very insightful, but there's definitely more out there in the literature about the problem with moral deference.
